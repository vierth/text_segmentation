{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Text\n",
    "This utility will take a folder of text files as input, tokenize the files, and return a new folder of files with spaces inserted between the words. It is designed to be as simple as possible, so all you need to do is specify the language you want to tokenize with, and the name of the folder you want to tokenize. This will create a new folder with the tokenized text\n",
    "\n",
    "## Setting variables\n",
    "You need to set a number of variables to make sure the code runs properly.\n",
    "\n",
    "### Language\n",
    "Below, inside the quotation marks, specify the language you are working with (this script is compatible with Chinese, Japanese, and Korean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Chinese\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus folder\n",
    "Inside the quotation marks below specify the name of the folder containing the files you want tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = \"chinese_corpus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output folder\n",
    "You can set the name of the folder where your results will be saved. I have set it up by default to be \"tokenized\" plus the name of your corpus directory. Note that anything inside this folder may be deleted when you start the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"tokenized_{corpus_dir}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code Itself\n",
    "That's it! Just run the codeblock below and it will tokenize your file for you. Note that this does try to install a tokenization library if your system does not already have it installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# make language lowercase for ease of entry\n",
    "language = language.lower()\n",
    "\n",
    "# depending on the language, import a tokenizer\n",
    "if language == \"chinese\":\n",
    "    \n",
    "    # try to import the library. install it if cannot find module\n",
    "    try:\n",
    "        import jieba\n",
    "    except ModuleNotFoundError:\n",
    "        import sys\n",
    "        !{sys.executable} -m pip install jieba\n",
    "        import jieba\n",
    "        \n",
    "elif language == \"japanese\":\n",
    "    try:\n",
    "        from fugashi import Tagger\n",
    "    except ModuleNotFoundError:\n",
    "        import sys\n",
    "        !{sys.executable} -m pip install fugashi\n",
    "        !{sys.executable} -m pip install unidic-lite\n",
    "        from fugashi import Tagger\n",
    "        \n",
    "    tagger = Tagger('-Owakati')\n",
    "    \n",
    "elif language == \"korean\":\n",
    "    try:\n",
    "        from konlpy.tag import Kkma\n",
    "    except ModuleNotFoundError:\n",
    "        import sys\n",
    "        !{sys.executable} -m pip install konlpy\n",
    "        from konlpy.tag import Kkma\n",
    "        \n",
    "    kkma = Kkma()\n",
    "    \n",
    "else:\n",
    "    print(\"please set language to Chinese, Japanese, or Korean (this is case sensitive)\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# check if the output directory exists. If it not, make it\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "\n",
    "# iterate through all files in the corpus directory\n",
    "for root, dirs, files in os.walk(corpus_dir):\n",
    "    for fname in files:\n",
    "        \n",
    "        # read file:\n",
    "        with open(os.path.join(root, fname), 'r', encoding='utf8') as rf:\n",
    "            text = rf.read()\n",
    "            \n",
    "        # tokenize the document and join by spaces if need be\n",
    "        if language == \"chinese\":\n",
    "            res = jieba.cut(text)\n",
    "            res = \" \".join(res)\n",
    "        elif language == \"japanese\":\n",
    "            res = ' '.join([w.surface for w in tagger(text)])\n",
    "            \n",
    "        elif language == \"korean\":\n",
    "            res = kkma.morphs(text)\n",
    "            res = \" \".join(res)\n",
    "        \n",
    "        with open(os.path.join(output_dir, fname), 'w', encoding='utf8') as wf:\n",
    "            wf.write(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
